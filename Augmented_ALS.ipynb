{"cells":[{"cell_type":"code","source":["class Augmented_ALS(object):  #use more sparse representation of entries - store as (i,j, weight)\n  \n  def optimal_partioning(self): #optimal partitioning to minimize wall-clock time per iteration\n      return ((8  , 8))\n    \n  @staticmethod\n  def get_size(model):\n      M = model.M\n      N = model.N\n      m = model.m\n      n = model.n\n      def _get_size(key):\n          m1 = M // m + 1*(M % m > 0)\n          n1 = N // n + 1*(N % n > 0) \n          if key[0] < (m - 1):\n              x1 = m1\n          else:\n              x1 = M - m1*(m - 1)    \n          if key[1] < (n - 1):\n              x2 = n1\n          else:\n              x2 = N - n1*(n - 1)\n          return (key, (x1, x2))\n      return _get_size\n  \n  @staticmethod\n  def create_edges(model):\n      M = model.M\n      N = model.N\n      m = model.m\n      n = model.n\n      alpha = model.alpha.value\n      def _create_rawentries(x):\n          m1 = M // m + 1*(M % m > 0)\n          n1 = N // n + 1*(N % n > 0) \n          weight = 1 + alpha*math.log10(1 + float(x[2]))\n          return ((x[0] // m1, x[1] // n1), [(x[0] % m1, x[1] % n1 , weight)])\n      return _create_rawentries   \n    \n    \n  @staticmethod\n  def partitioning(model):\n      n = model.n     \n      def _partitioning(key):\n          return key[0]*n + key[1]\n      return _partitioning\n \n  def create_entries(self):   \n      frames = sc.parallelize([(x,y) for x in range(self.m) for y in range(self.n)])\n      frames = frames.map(Augmented_ALS.get_size(self)).partitionBy(self.numpartitions, Augmented_ALS.partitioning(self)).persist() \n      if self.fraction is not None:\n          dev = self.ratings.sample(False, self.fraction).persist()\n          train = self.ratings.subtract(dev)                       \n      else:\n          train = self.ratings\n          \n      if self.ineligible is not None:\n          ineligible = self.ineligible_ratings.map(lambda x: (x[0], x[1], float('-inf'))) # -ve weight indicates ineligibility\n          train = train.union(ineligible)\n      rw_entries = train.map(Augmented_ALS.create_edges(self)).reduceByKey(lambda x, y : x + y)      \n      train = frames.leftOuterJoin(rw_entries).cache()       \n      if self.fraction is not None:\n          rw_dev = dev.map(Augmented_ALS.create_edges(self)).reduceByKey(lambda x, y : x + y)\n          test = frames.leftOuterJoin(rw_dev).cache()\n      else:\n          test = None         \n      frames.unpersist()\n      return train, test\n   \n  @staticmethod\n  def flatten_customerfeatures(model):\n      M = model.M      \n      m = model.m\n      n = model.n\n      def _flatten(x):\n          m1 = M // m + 1*(M % m > 0) \n          return [((int(x[0]) // m1, i),  (int(x[0]) % m1, x[1:])) for i in range(n)]\n      return _flatten\n    \n  @staticmethod\n  def flatten_productfeatures(model):\n      N = model.N      \n      m = model.m\n      n = model.n\n      def _flatten(x):\n          n1 = N // n + 1*(N % n > 0) \n          return [((i, int(x[0]) // n1),  (int(x[0]) % n1, x[1:])) for i in range(m)]\n      return _flatten\n    \n  @staticmethod\n  def create_vecs(values):  \n      sorted_values = sorted(values, key = (lambda x: x[0]))\n      return np.array([t[1] for t in  sorted_values])\n  \n  @staticmethod\n  def generate_factors(model):\n      f = model.f\n      def _generate(key):\n          return (int(key[0]), np.random.randn(f + 1))\n      return _generate\n  \n  @staticmethod\n  def propagate_customerfactors(model):\n      M = model.M\n      m = model.m\n      n = model.n\n      def _prop(value):\n          m1 = M // m + 1*(M % m > 0)\n          return [((value[0] // m1, i), (value[0] % m1, value[1])) for i in range(n)]\n      return _prop\n    \n  @staticmethod\n  def propagate_productfactors(model):   \n      N = model.N\n      m = model.m\n      n = model.n\n      def _prop(value):\n          n1 = N // n + 1*(N % n > 0)\n          return [((i, value[0] // n1), (value[0] % n1, value[1])) for i in range(m)]\n      return  _prop                     \n\n  def load_customerfeatures(self):\n      c_features = self.c_features_rdd.flatMap(Augmented_ALS.flatten_customerfeatures(self)).groupByKey().mapValues(Augmented_ALS.create_vecs)\n      A = self.entries.join(c_features).mapValues(lambda x: x[1]).cache()      \n      return A\n  \n  def load_productfeatures(self):\n      p_features = self.p_features_rdd.flatMap(Augmented_ALS.flatten_productfeatures(self)).groupByKey().mapValues(Augmented_ALS.create_vecs)\n      B = self.entries.join(p_features).mapValues(lambda x: x[1]).cache()      \n      return B\n    \n  def initialize_customer_factors(self):\n      c_factors = self.c_features_rdd.map(Augmented_ALS.generate_factors(self))\n      c_factors = c_factors.flatMap(Augmented_ALS.propagate_customerfactors(self)).groupByKey().mapValues(Augmented_ALS.create_vecs)\n      X = self.entries.join(c_factors).mapValues(lambda x: x[1]).cache()      \n      return X\n    \n  def initialize_product_factors(self):\n      p_factors = self.p_features_rdd.map(Augmented_ALS.generate_factors(self))\n      p_factors = p_factors.flatMap(Augmented_ALS.propagate_productfactors(self)).groupByKey().mapValues(Augmented_ALS.create_vecs)\n      Y = self.entries.join(p_factors).mapValues(lambda x: x[1]).cache()      \n      return Y\n    \n    \n  def intitialize_embeddings(self):\n      G = np.random.randn(self.k, self.f)\n      g = np.random.randn(self.k) \n      D = np.random.randn(self.l, self.f)\n      d = np.random.randn(self.l) \n      G_rdd = self.entries.mapValues(lambda x: G).cache()     \n      g_rdd = self.entries.mapValues(lambda x: g).cache()     \n      D_rdd = self.entries.mapValues(lambda x: D).cache()      \n      d_rdd = self.entries.mapValues(lambda x: d).cache()      \n      return G_rdd, g_rdd, D_rdd, d_rdd          \n\n    \n  @staticmethod  \n  def index_customer_msg(model):\n      M = model.M\n      m = model.m\n      def _index(msg):\n          m1 = M // m + 1*(M % m > 0)\n          start = msg[0][0]*m1\n          end = msg[1].shape[0] + start\n          ids = np.expand_dims(np.asarray(list(range(start, end))), axis = 1)\n          return np.hstack((ids, msg[1]))  \n      return _index\n     \n    \n  def customer_msg_to_rdd(self, msgs):\n      return msgs.map(Augmented_ALS.index_customer_msg(self)).flatMap(lambda x: list(x)).map(lambda x: (int(x[0]) , x[1:]))  \n  \n  @staticmethod  \n  def index_product_msg(model):\n      N = model.N\n      n = model.n\n      def _index(msg):\n          n1 = N // n + 1*(N % n > 0)\n          start = msg[0][1]*n1\n          end = msg[1].shape[0] + start\n          ids = np.expand_dims(np.asarray(list(range(start, end))), axis = 1)\n          return np.hstack((ids, msg[1]))       \n      return _index\n    \n    \n  def product_msg_to_rdd(self, msgs):\n      return msgs.map(Augmented_ALS.index_product_msg(self)).flatMap(lambda x: list(x)).map(lambda x: (int(x[0]) , x[1:]))  \n  \n  \n  def update_embedding_factors(self, vec, Lambda, k):\n      f_ = self.f*k + k\n      I_G = Lambda*np.eye(k*self.f + k)\n      x1 = vec[:-f_].reshape(f_,-1)\n      x2 = np.expand_dims(vec[-f_:], axis = 1)\n      temp = inv(x1 + I_G)\n      updates = np.squeeze(np.matmul(temp, x2))\n      G = np.reshape(updates[:k*self.f],(k,self.f))\n      g = updates[-k:]    \n      return G, g\n             \n  @staticmethod\n  def update_customer_factors(model):\n      Lambda = model.Lambda_x\n      f_ = model.f + 1\n      update = model.vec_update \n      def _update(vec):        \n          return update(vec, Lambda, f_)      \n      return _update\n\n  @staticmethod\n  def vec_update(vec, Lambda, f_):\n      x1 = vec[:-f_].reshape(f_,-1)\n      x2 = np.expand_dims(vec[-f_:], axis = 1)\n      temp = inv(x1 + Lambda*np.eye(f_))\n      return np.squeeze(np.matmul(temp, x2))\n    \n  @staticmethod\n  def update_product_factors(model):\n      Lambda = model.Lambda_y\n      f_ = model.f + 1\n      update = model.vec_update      \n      def _update(vec):        \n          return update(vec, Lambda, f_)      \n      return _update\n            \n            \n  def update_Xblock(self):\n      self.Xblock.unpersist()\n      D_d = self.D.join(self.d)\n      G_g = self.G.join(self.g)\n      embeddings = D_d.join(G_g)\n      features = self.A.join(self.B)\n      msgs = self.entries.join(embeddings).join(features).join(self.Yblock).mapValues(Augmented_ALS.customerfactorupdate(self))\n      msgs2 = self.customer_msg_to_rdd(msgs).coalesce(self.numpartitions)\n      reduced = msgs2.reduceByKey(lambda x, y : x + y)\n      print (reduced.getNumPartitions())\n      \n      c1_factors = reduced.mapValues(Augmented_ALS.update_customer_factors(self)).flatMap(Augmented_ALS.propagate_customerfactors(self))\n      \n      print (c1_factors.getNumPartitions())\n      c1 = c1_factors.groupByKey().mapValues(Augmented_ALS.create_vecs)   \n     \n      self.Xblock = self.entries.join(c1).mapValues(lambda x: x[1]).cache()      \n      self.Xblock.count()      \n\n      \n  def update_Gg_block(self):\n      self.G.unpersist()\n      self.g.unpersist()\n      blocks = self.Xblock.join(self.Yblock)\n      features = self.A.join(self.B)\n      frame = self.entries.join(blocks).join(features).join(self.D.join(self.d)).mapValues(Augmented_ALS.userembedding_update)\n      updated = frame.map(lambda x: x[1]).reduce(lambda x, y: x + y)\n      G, g =  self.update_embedding_factors(updated, self.Lambda_G, self.k)\n      self.G = self.entries.mapValues(lambda x: G).cache()\n      self.g = self.entries.mapValues(lambda x: g).cache()    \n      self.G.count()\n      self.g.count()     \n      \n  def update_Dd_block(self):\n      self.D.unpersist()\n      self.d.unpersist()\n      blocks = self.Xblock.join(self.Yblock)\n      features = self.A.join(self.B)\n      frame = self.entries.join(blocks).join(features).join( self.G.join(self.g)).mapValues(Augmented_ALS.productembedding_update)\n      updated = frame.map(lambda x: x[1]).reduce(lambda x, y: x + y)\n      D, d =  self.update_embedding_factors(updated, self.Lambda_D, self.l)\n      self.D = self.entries.mapValues(lambda x: D).cache()\n      self.d = self.entries.mapValues(lambda x: d).cache()    \n          \n  def update_Yblock(self):\n      self.Yblock.unpersist()\n      D_d = self.D.join(self.d)\n      G_g = self.G.join(self.g)\n      embeddings = D_d.join(G_g)\n      features = self.A.join(self.B)\n      msgs = self.entries.join(embeddings).join(features).join(self.Xblock).mapValues(Augmented_ALS.productfactorupdate(self))\n      msgs2 = self.product_msg_to_rdd(msgs).coalesce(self.numpartitions)\n      reduced = msgs2.reduceByKey(lambda x, y: x + y)\n      print (reduced.getNumPartitions())      \n      \n      updated = reduced.mapValues(Augmented_ALS.update_product_factors(self))\n      p1_factors = updated.flatMap(Augmented_ALS.propagate_productfactors(self))\n      print (p1_factors.getNumPartitions())\n      \n      p1 = p1_factors.groupByKey().mapValues(Augmented_ALS.create_vecs)     \n      self.Yblock = self.entries.join(p1).mapValues(lambda x: x[1]).cache()\n      self.Yblock.count()\n     \n              \n  def train(self, iterations = 10): \n      for itr in range(0, iterations):\n          print ('%s %s' % ('Starting iteration #: ', itr + 1)) \n      \n          start = datetime.datetime.now()\n          start0 = start\n          self.update_Xblock()\n          el = datetime.datetime.now() - start\n          print ('%s %s' % ('Finshed Xblock with time #: ', el)) \n          \n          start = datetime.datetime.now()          \n          self.update_Yblock()\n          el = datetime.datetime.now() - start\n          print ('%s %s' % ('Finshed Yblock with time #: ', el)) \n          \n          \n          start = datetime.datetime.now()            \n          self.update_Gg_block()\n          el = datetime.datetime.now() - start\n          print ('%s %s' % ('Finshed Ggblock with time #: ', el))\n          \n          start = datetime.datetime.now() \n          self.update_Dd_block()\n          print str(datetime.datetime.now() - start)\n          el = datetime.datetime.now() - start\n          print ('%s %s' % ('Finshed Ddblock with time #: ', el))\n          el = datetime.datetime.now() - start0\n          print ('%s %s' % ('Total time for iteration #: ', el))\n          \n      \n  @staticmethod\n  def local_props(model):       \n      m1 = model.M // model.m + 1*(model.M % model.m > 0)\n      def _index(msg):          \n          key, _ = msg[0]\n          dev = msg[1][0][1]\n          m, n = msg[1][0][0]\n          propensities = msg[1][1]\n          start = m1*key\n          def update_msg_axis(x, n,  dev, propensities, start): #update as per v2\n              props = propensities[x, :]\n              if dev is not None:\n                  indices = [item[1] for item in dev if item[0] == x]\n                  props_ = list(props[indices])                 \n              else:\n                  props_ = []\n              return (start + x, props_)     \n          return  map(lambda x: update_msg_axis(x, n,  dev, propensities, start), list(range(m)))  \n      return _index\n    \n  def test_props(self): #create partioned RDD of (user, list of test props) \n      self.propensities  = self.generate_propensities().persist()\n      local_props = self.dev.join(self.propensities).flatMap(Augmented_ALS.local_props(self))\n      global_props  = local_props.reduceByKey(lambda x, y: x + y).flatMap(Augmented_ALS.propagate_customerfactors(self)).groupByKey()      \n      return self.entries.join(global_props).mapValues(lambda x : list(x[1]))\n    \n \n  def get_AUC2(self):\n    global_props = self.test_props()     \n    globe = self.entries.join(self.dev).join(self.propensities).join(global_props).mapValues(Augmented_ALS.AUC2)\n    AUC = globe.flatMap(lambda x: x[1]).reduceByKey(lambda x, y: (x[0]+ y[0], x[1]+ y[1])).filter(lambda x: x[1][1] > 0).persist()\n    count = AUC.count()\n    result = AUC.map(lambda x: x[1][0]/x[1][1]).reduce(lambda a, b: a + b)\n    self.propensities.unpersist()\n    AUC.unpersist()\n    return result/count\n      \n  \n    \n  @staticmethod\n  def AUC2(values):\n      #self.entries.join(self.dev).join(self.propensities).join(global_props).mapValues(Augmented_ALS.AUC2)\n      entries = values[0][0][0][1]\n      dev = values[0][0][1][1]\n      propensities = values[0][1]\n      global_props = values[1]\n      m, n = values[0][0][0][0]\n      assert n == propensities.shape[1], \"incompatible dimensions\"\n      assert m == propensities.shape[0], \"incompatible dimensions\"\n      def AUC_axis(x, entries, globe, propensities, n):\n          props = propensities[x, :]\n          if entries is not None:\n              a = [item[1] for item in entries if item[0] == x and item[2] > 0 ]   \n          else:\n              a = []\n          if dev is not None:\n              b = [item[1] for item in dev if item[0] == x]\n          else:\n              b = []\n         \n          a.extend(b)          \n          l = [item[1] for item in global_props if item[0] == x]\n          candidate_props = [item for sublist in l for item in sublist]\n          \n          indices = [x for x in list(range(n)) if x not in a]\n          print (indices)         \n          target_props = props[indices]\n         \n          l = len(target_props)\n          result = 0.  \n          count = 0\n          for prop in candidate_props:\n              result += np.sum(target_props < prop)\n          count = l*len(candidate_props)\n          return (x, (result, count))\n      return  map(lambda x: AUC_axis(x, entries, dev, propensities, n), list(range(m)))      \n    \n  def generate_propensities(self): #generates dense matrix of customer-item propensities\n      D_d = self.D.join(self.d)\n      G_g = self.G.join(self.g)\n      embeddings = D_d.join(G_g)\n      features = self.A.join(self.B)\n      factors = self.Xblock.join(self.Yblock)\n      return self.entries.join(embeddings).join(features).join(factors).mapValues(Augmented_ALS.compute_propensities)   \n    \n  @staticmethod \n  def compute_propensities(value):\n      #extract  relavent values\n      X = value[1][0][:, :-1]\n      Y = value[1][1][:, :-1]\n      Bias_user = value[1][0][:, -1]  \n      Bias_item = value[1][1][:, -1]        \n      A = value[0][1][0]\n      B = value[0][1][1]\n      D = value[0][0][1][0][0]\n      d = value[0][0][1][0][1]      \n     \n      G = value[0][0][1][1][0]\n      g = value[0][0][1][1][1]\n      \n      Bu = np.expand_dims(Bias_user, axis = 1)\n      Bi = np.expand_dims(Bias_item, axis = 0)\n      \n      user_factors = np.matmul(A,G) + X\n      product_factors = np.matmul(B,D) + Y\n      temp1 =  np.matmul(user_factors, product_factors.T)\n      temp2 = np.expand_dims(np.matmul(A,g), axis = 1)\n      temp3 = np.expand_dims(np.matmul(B,d), axis = 0)\n      propensities = temp1 + temp2 + temp3 + Bu + Bi      \n      return propensities      \n    \n      \n  def __init__(self, ratings_rdd, c_features, p_features, ineligible_edges = None, alpha = 41, f = 3, Lambda_x = 2, Lambda_y = 2, Lambda_G = 2, Lambda_D = 2, fraction = 0.2):\n      self.numpartitions  = 64   #placeholder; will later obtain from spark context\n      self.ratings = ratings_rdd\n      self.ineligible = ineligible_edges\n      self.N = ratings_rdd.map(lambda x: x[1]).top(1)[0] + 1\n      self.M = ratings_rdd.map(lambda x: x[0]).top(1)[0] + 1\n      m, n = self.optimal_partioning()  \n      self.m = m\n      self.n = n\n      self.alpha = sc.broadcast(alpha)\n      self.fraction = fraction\n      self.entries, self.dev = self.create_entries() #partition purchases, weights\n      self.entries.count()\n      self.c_features_rdd = c_features\n      self.A  = self.load_customerfeatures() #partition customer features\n      self.k = self.A.lookup((0,0))[0].shape[1]        \n      self.p_features_rdd = p_features\n      self.B  = self.load_productfeatures() #partition product features  \n      self.l = self.B.lookup((0,0))[0].shape[1]   \n      self.f = f\n      self.Xblock = self.initialize_customer_factors()     \n      self.Xblock.count()\n      self.Yblock = self.initialize_product_factors()    \n      self.Yblock.count()\n      self.G, self.g, self.D, self.d  = self.intitialize_embeddings()          \n      self.Lambda_x = Lambda_x\n      self.Lambda_y = Lambda_y\n      self.Lambda_G = Lambda_G\n      self.Lambda_D = Lambda_D     \n\n \n  \n  @staticmethod \n  def userembedding_update(value):\n      #extract  relavent values\n      X = value[0][0][1][0][:, :-1]\n      Y = value[0][0][1][1][:, :-1]\n      Bias_item = value[0][0][1][1][:, -1] \n      Bias_user = value[0][0][1][0][:, -1]       \n      A = value[0][1][0]\n      B = value[0][1][1]\n      D = value[1][0]\n      d = value[1][1]      \n      \n      ''''generate R and C '''''\n      entries = value[0][0][0][1]\n      m, n = value[0][0][0][0]\n      assert ((n == B.shape[0]) and (m == A.shape[0])), \"dimension incompatibility\"\n      assert ((n == Y.shape[0]) and (m == X.shape[0])), \"dimension incompatibility\"\n      \n      I = np.array([x[0] for x in entries])\n      J = np.array([x[1] for x in entries])\n      V = np.array([x[2] for x in entries], dtype= float)       \n      R = np.zeros((m,n))      \n      C = sparse.coo_matrix((V,(I,J)), shape=(m,n)).toarray()\n      R[C > 0] = 1      \n      C[C == 0] = 1     \n           \n      \n      k = A.shape[1]\n      l = B.shape[1]\n      \n      B_ = np.matmul(B, D) + Y\n      Y_ = np.reshape(np.swapaxes(np.tensordot(A, B_, axes = 0), 1,2), (m, n, -1))\n      \n      X_ = np.matmul(X, B_.T)    \n      Bu = np.expand_dims(Bias_user, axis = 1)\n      Bi = np.expand_dims(Bias_item, axis = 0)\n      \n      R_adj = R  -  Bi -  np.expand_dims(np.matmul(B, d), axis = 0)  - Bu - X_\n      \n      A_ = np.zeros((m,n,k)) + np.expand_dims(A, axis = 1)\n      Y_ = np.concatenate((Y_, A_), axis = 2)\n    \n      Y_ = np.reshape(Y_, (m*n, -1))\n      C_ = np.squeeze(np.reshape(C, (m*n, -1)))\n      R_adj = np.reshape(R_adj, (m*n, -1))\n      Filter = C_\n      \n      C_adj = C_[Filter > -1]    \n      n_adj = C_adj.size    \n    \n      C_ui = sparse.spdiags(C_adj, 0, C_adj.size, C_adj.size).tocsr()\n      p = R_adj[Filter > -1]\n      Y_ = Y_[Filter > -1, :]      \n           \n      term1 = C_ui* sparse.csr_matrix(Y_).toarray()\n      term2 = C_ui* sparse.csr_matrix(p).toarray()\n      msg1 = np.matmul(Y_.T, term1).ravel() \n      msg2 = np.squeeze(np.matmul(Y_.T, term2).T)\n      return np.hstack((msg1, msg2)) #returns the flatenned matrices          \n            \n  @staticmethod \n  def productembedding_update(value):      \n      #extract  relavent values\n      X = value[0][0][1][0][:, :-1]\n      Y = value[0][0][1][1][:, :-1]\n      Bias_item = value[0][0][1][1][:, -1] \n      Bias_user = value[0][0][1][0][:, -1]       \n      A = value[0][1][0]\n      B = value[0][1][1]\n      G = value[1][0]\n      g = value[1][1]\n      m = X.shape[0] \n      n = Y.shape[0]\n      \n      ''''generate R and C '''''\n      entries = value[0][0][0][1]\n      m, n = value[0][0][0][0]\n      assert ((n == B.shape[0]) and (m == A.shape[0])), \"dimension incompatibility\"\n      assert ((n == Y.shape[0]) and (m == X.shape[0])), \"dimension incompatibility\"\n      \n      I = np.array([x[0] for x in entries])\n      J = np.array([x[1] for x in entries])\n      V = np.array([x[2] for x in entries], dtype= float)       \n      R = np.zeros((n,m))      \n      C = sparse.coo_matrix((V,(J,I)), shape=(n,m)).toarray()\n      R[C > 0] = 1      \n      C[C == 0] = 1    \n      \n      k = A.shape[1]\n      l = B.shape[1]\n      \n      A_ = np.matmul(A, G) + X\n      X_ = np.reshape(np.swapaxes(np.tensordot(B, A_, axes = 0), 1,2), (n, m, -1))\n      \n      Y_ = np.matmul(Y, A_.T)    \n      Bi = np.expand_dims(Bias_item, axis = 1)\n      Bu = np.expand_dims(Bias_user, axis = 0)\n      \n      R_adj = R  -  Bu -  np.expand_dims(np.matmul(A, g), axis = 0)  - Bi - Y_\n      \n      B_ = np.zeros((n,m,l)) + np.expand_dims(B, axis = 1)\n      X_ = np.concatenate((X_, B_), axis = 2)\n    \n      X_ = np.reshape(X_, (n*m, -1))\n      C_ = np.squeeze(np.reshape(C, (n*m, -1)))\n      R_adj = np.reshape(R_adj, (n*m, -1))\n      Filter = C_\n      \n      C_adj = C_[Filter > -1]    \n      n_adj = C_adj.size    \n     \n      C_ui = sparse.spdiags(C_adj, 0, C_adj.size, C_adj.size).tocsr()\n      p = R_adj[Filter > -1]\n      X_ = X_[Filter > -1, :]      \n           \n      term1 = C_ui* sparse.csr_matrix(X_).toarray()\n      term2 = C_ui* sparse.csr_matrix(p).toarray()\n      msg1 = np.matmul(X_.T, term1).ravel() \n      msg2 = np.squeeze(np.matmul(X_.T, term2).T)\n      return np.hstack((msg1, msg2)) #returns the flatenned matrices \n \n    \n  @staticmethod\n  def update_msg_axis(index, flag, entries, A, Y_, Bias_item, B, D, d, G, g, n, alpha): \n          k = G.shape[0]          \n          dim = n  \n        \n          #create the pu and filter matrices\n          p_u = np.zeros(dim)\n          if flag: #flag indicates we are updating for users             \n              ents = [(x[1], x[2]) for x in entries if x[0] == index]\n          else: #we are updating for items             \n              ents = [(x[0], x[2]) for x in entries if x[1] == index]          \n          \n          inds = [x[0] for x in ents if x[1] > 0]\n          p_u[inds] = 1\n          \n          Filter = 1 + np.zeros(dim)\n          inds = [x[0] for x in ents]\n          vals = [x[1] for x in ents]\n          Filter[inds] = vals\n                   \n          p_u = p_u[Filter > 0]\n          \n          alpha_u = A[index, :]       \n    \n          term1 = np.matmul(Y_, np.matmul(alpha_u, G))\n          term1 = term1[Filter > -1]            \n          term2 = np.matmul(B, d)\n          term2 = term2[Filter > -1]            \n          term3 = np.matmul(g, alpha_u)            \n          term4 = Bias_item[Filter > -1]            \n          p_u = p_u  - term1 - term2  - term3 - term4\n          \n          n_adj = p_u.size         \n          temp = Filter[Filter > -1]\n          c_u = temp*np.eye(n_adj) \n            \n          ones = np.ones((n_adj, 1))   \n          Yadj = Y_[Filter > -1, :]            \n          Yadj = np.concatenate((Yadj, ones), axis = 1)   \n          \n          msg1 = np.matmul(Yadj.T, (np.matmul(c_u,Yadj))).ravel()         \n          msg2 = np.matmul(Yadj.T, np.matmul(c_u, p_u)).T          \n          return np.hstack((msg1, msg2)) #returns the flatenned matrices of dimenion 1 X [(f+1)*(f+1) +f+1]      \n  \n  \n  @staticmethod \n  def customerfactorupdate(model):       \n      udateX = model.update_msg_axis\n      alpha = model.alpha\n      def _update(value):\n          #extract  relavent values\n          Y = value[1][:, :-1]\n          Bias_item =value[1][:, -1]  \n          A = value[0][1][0]\n          B = value[0][1][1]\n          D = value[0][0][1][0][0]\n          B_ = np.matmul(B, D)\n          Y_ = Y + B_\n          d = value[0][0][1][0][1]\n          G = value[0][0][1][1][0]\n          g = value[0][0][1][1][1]\n          ratings = value[0][0][0][1]   #entries of weighted ratings                    \n          m, n = value[0][0][0][0]     \n          assert ((n == B.shape[0]) and (m == A.shape[0])), \"dimension incompatibility\"\n          return  np.array(map(lambda index: udateX(index, True, ratings, A, Y_, Bias_item, B, D, d, G, g, n, alpha.value), list(range(m))))\n      return _update\n    \n  @staticmethod \n  def productfactorupdate(model):        \n      udateY = model.update_msg_axis\n      alpha = model.alpha\n      def _update(value):\n      #extract  relavent values\n          X = value[1][:, :-1]\n          Bias_user =value[1][:, -1]  \n          A = value[0][1][0]\n          B = value[0][1][1]\n          D = value[0][0][1][0][0]\n          G = value[0][0][1][1][0]\n          A_ = np.matmul(A, G)\n          X_ = X + A_\n          d = value[0][0][1][0][1]     \n          g = value[0][0][1][1][1]\n           \n          ratings = value[0][0][0][1]   #entries of weighted ratings                    \n          m, n = value[0][0][0][0]     \n          assert ((n == B.shape[0]) and (m == A.shape[0])), \"dimension incompatibility\"\n          return  np.array(map(lambda index: udateY(index, False, ratings, B, X_, Bias_user, A, G, g, D, d, m, alpha.value), list(range(n))))\n      return _update"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import numpy as np\nfrom numpy.linalg import inv\nfrom scipy import sparse\nimport datetime\nimport random as rd\nimport math\n#sc.setCheckpointDir(\"/FileStore7\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data = sc.textFile(\"/FileStore/tables/wkmurkxo1506499092834/Purchases.csv\")\nheader = data.first() #extract header\nratings = data.filter(lambda row : row != header).map(lambda l: l.split(\",\")).map(lambda rating: (int(rating[0]) , int(rating[1]),  )).distinct() #remove duplicate entries\n\nratings = ratings.map(lambda x: (x[0], x[1], rd.lognormvariate(3, 1))).persist() #add weights to ratings\n\ncustomer_features = sc.textFile(\"FileStore/tables/elwqnkv61506837540758/Customer_Features.csv\")\nheader = customer_features.first() #extract header\ncustomer_features = customer_features.filter(lambda row : row != header) \n#customer_features = customer_features.map(lambda l: l.split(\",\")).map(lambda x: map(float, x)).filter(lambda x: (x[0] < 10)).persist()\ncustomer_features = customer_features.map(lambda l: l.split(\",\")).map(lambda x: map(float, x)).persist()\nprint customer_features.count()\n\nproduct_features = sc.textFile(\"/FileStore/tables/elwqnkv61506837540758/Product_Features.csv\")\nheader = product_features .first() #extract header\nproduct_features  = product_features .filter(lambda row : row != header) \n#product_features  = product_features .map(lambda l: l.split(\",\")).map(lambda x: map(float, x)).filter(lambda x: (x[0] < 8)).persist()\nproduct_features  = product_features .map(lambda l: l.split(\",\")).map(lambda x: map(float, x)).persist()\nprint product_features.count()\n\nratings.take(3)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["r3 = Augmented_ALS(ratings, customer_features, product_features)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["r3.train(10)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["r3.get_AUC2()"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"Augmented_ALS","notebookId":3459389966286829},"nbformat":4,"nbformat_minor":0}
